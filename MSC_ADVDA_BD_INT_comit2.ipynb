{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "<h2 align=\"center\" style=\"background-color:#2c3e54;color:#ecf0f1;border-radius: 8px; padding:15px\">Time-Series Forecasting in Financial Markets: Integrating Attention Mechanisms with Traditional Neural Networks for High-Frequency Trading Data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Table of Contents**\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "   - Research Overview\n",
    "   - Objectives\n",
    "   - Data Source and Storage\n",
    "- [Install and Import Required Libraries](#Install-and-Import-Required-Libraries)\n",
    "- [Download and Load Dataset](#Download-and-Load-Dataset)\n",
    "- [Data Exploration](#Data-Exploration)\n",
    "   - View First Five Rows\n",
    "   - Inspect Shape\n",
    "   - Investigate Missing data, duplicates and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T08:23:52.241021Z",
     "iopub.status.busy": "2025-02-28T08:23:52.240719Z",
     "iopub.status.idle": "2025-02-28T08:23:52.247331Z",
     "shell.execute_reply": "2025-02-28T08:23:52.246134Z",
     "shell.execute_reply.started": "2025-02-28T08:23:52.240997Z"
    }
   },
   "source": [
    "<h3 style=\"background-color:#2c3e54;color:#ecf0f1;border-radius: 8px; padding:15px\">Introduction</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T08:26:12.008598Z",
     "iopub.status.busy": "2025-02-28T08:26:12.008322Z",
     "iopub.status.idle": "2025-02-28T08:26:12.011797Z",
     "shell.execute_reply": "2025-02-28T08:26:12.010942Z",
     "shell.execute_reply.started": "2025-02-28T08:26:12.008574Z"
    }
   },
   "source": [
    "### Objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source and Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color:#2c3e54;color:#ecf0f1;border-radius: 8px; padding:15px\">Install and Import Required Libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -q yfinance\n",
    "!pip install -q pyspark pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType, DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color:#2c3e54;color:#ecf0f1;border-radius: 8px; padding:15px\">Download and Load Dataset</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and Store Finance Data using Y-Finance API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PySpark session\n",
    "spark = SparkSession.builder.appName(\"StockDataStorage\").getOrCreate()\n",
    "\n",
    "# Define stock symbols\n",
    "stocks = [\"GOOG\", \"AMZN\", \"MSFT\", \"PYPL\", \"TSLA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema (Date as String initially, to convert later)\n",
    "schema = StructType([\n",
    "    StructField(\"Date\", StringType(), True),  # Initially store as String\n",
    "    StructField(\"Open\", DoubleType(), True),\n",
    "    StructField(\"High\", DoubleType(), True),\n",
    "    StructField(\"Low\", DoubleType(), True),\n",
    "    StructField(\"Close\", DoubleType(), True),\n",
    "    StructField(\"Volume\", IntegerType(), True),\n",
    "    StructField(\"Dividends\", DoubleType(), True),\n",
    "    StructField(\"Stock_Splits\", DoubleType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_path = \"pyspark_stock_data/\"\n",
    "\n",
    "# Fetch, store, and load data\n",
    "for stock in stocks:\n",
    "    # Fetch max available stock data with 1-day interval\n",
    "    dat = yf.Ticker(stock)\n",
    "    df = dat.history(period=\"max\", interval=\"1d\")\n",
    "\n",
    "    # Reset index to move Date column\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    # Convert Date column to string (YYYY-MM-DD format)\n",
    "    df[\"Date\"] = df[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Convert to PySpark DataFrame\n",
    "    spark_df = spark.createDataFrame(df, schema=schema)\n",
    "\n",
    "    # Convert Date column to actual DateType()\n",
    "    spark_df = spark_df.withColumn(\"Date\", spark_df[\"Date\"].cast(DateType()))\n",
    "\n",
    "    # Save as Parquet\n",
    "    stock_path = f\"{storage_path}{stock}.parquet\"\n",
    "    spark_df.write.mode(\"overwrite\").parquet(stock_path)\n",
    "    \n",
    "    print(f\"Stored {stock} data at {stock_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Saved PySpark Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into separate variables\n",
    "goog_df = spark.read.parquet(f\"{storage_path}GOOG.parquet\")\n",
    "amzn_df = spark.read.parquet(f\"{storage_path}AMZN.parquet\")\n",
    "msft_df = spark.read.parquet(f\"{storage_path}MSFT.parquet\")\n",
    "pypl_df = spark.read.parquet(f\"{storage_path}PYPL.parquet\")\n",
    "tsla_df = spark.read.parquet(f\"{storage_path}TSLA.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color:#2c3e54;color:#ecf0f1;border-radius: 8px; padding:15px\">Data Exploration</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
