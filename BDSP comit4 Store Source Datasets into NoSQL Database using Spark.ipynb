{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "923bcb38",
   "metadata": {},
   "source": [
    "<h1 align=\"center\" style=\"background-color:#2c3e54;color:#ecf0f1;border-radius: 8px; padding:15px\">MSc in Data Analytics: Big Data Storage and Processing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d805034",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "    - [Assessment Overview](#Assessment-Overview)\n",
    "    - [Project Summary](#Project-Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e0502b",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#2c3e54;color:#ecf0f1;border-radius: 8px; padding:15px\">Introduction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb5eed",
   "metadata": {},
   "source": [
    "### **Assessment Overview**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef855670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db0a1465",
   "metadata": {},
   "source": [
    "### Project Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184e2659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bce8a253",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#2c3e54;color:#ecf0f1;border-radius: 8px; padding:15px\">Install and Import Required Libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "302fe9f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:39:32.666380Z",
     "iopub.status.busy": "2025-07-14T18:39:32.665981Z",
     "iopub.status.idle": "2025-07-14T18:39:32.672666Z",
     "shell.execute_reply": "2025-07-14T18:39:32.671542Z",
     "shell.execute_reply.started": "2025-07-14T18:39:32.666350Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -q pyspark pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "709169ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:39:32.707810Z",
     "iopub.status.busy": "2025-07-14T18:39:32.707395Z",
     "iopub.status.idle": "2025-07-14T18:39:33.477834Z",
     "shell.execute_reply": "2025-07-14T18:39:33.476330Z",
     "shell.execute_reply.started": "2025-07-14T18:39:32.707774Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b8b63c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:39:33.481458Z",
     "iopub.status.busy": "2025-07-14T18:39:33.480131Z",
     "iopub.status.idle": "2025-07-14T18:39:33.487510Z",
     "shell.execute_reply": "2025-07-14T18:39:33.486279Z",
     "shell.execute_reply.started": "2025-07-14T18:39:33.481407Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s — %(levelname)s %(message)s', force=True)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Disable warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1b1730",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#2c3e54;color:#ecf0f1;border-radius: 8px; padding:15px\">Define Data Paths</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "264f9050",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:39:33.489524Z",
     "iopub.status.busy": "2025-07-14T18:39:33.489031Z",
     "iopub.status.idle": "2025-07-14T18:39:33.511870Z",
     "shell.execute_reply": "2025-07-14T18:39:33.510572Z",
     "shell.execute_reply.started": "2025-07-14T18:39:33.489471Z"
    }
   },
   "outputs": [],
   "source": [
    "STOCKPRICE_FOLDER = \"/kaggle/input/stock-tweet-and-price/stock-tweet-and-price/stockprice\"\n",
    "STOCKTWEET_CSV = \"/kaggle/input/stock-tweet-and-price/stock-tweet-and-price/stocktweet/stocktweet.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c158e95c",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#2c3e54;color:#ecf0f1;border-radius: 8px; padding:15px\">Set MongoDB Connection</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8393259",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:39:33.514598Z",
     "iopub.status.busy": "2025-07-14T18:39:33.514017Z",
     "iopub.status.idle": "2025-07-14T18:39:33.710304Z",
     "shell.execute_reply": "2025-07-14T18:39:33.708598Z",
     "shell.execute_reply.started": "2025-07-14T18:39:33.514560Z"
    }
   },
   "outputs": [],
   "source": [
    "user_secrets = UserSecretsClient()\n",
    "mongodb_uri = user_secrets.get_secret(\"mongodb-atlas-uri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e8afdbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:39:33.712587Z",
     "iopub.status.busy": "2025-07-14T18:39:33.712223Z",
     "iopub.status.idle": "2025-07-14T18:39:34.454531Z",
     "shell.execute_reply": "2025-07-14T18:39:34.453590Z",
     "shell.execute_reply.started": "2025-07-14T18:39:33.712558Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 18:39:34,449 — INFO Connected to MongoDB database: stock_analytics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB version: 8.0.11\n"
     ]
    }
   ],
   "source": [
    "def create_mongodb_connection(uri, db_name='stock_analytics'):\n",
    "    \"\"\"\n",
    "    Connect to MongoDB and return database instance\n",
    "    \n",
    "    Args:\n",
    "        uri (str): Database URI\n",
    "        db_name (str): Database name\n",
    "    \n",
    "    Returns:\n",
    "        pymongo.database.Database: MongoDB database instance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = MongoClient(uri)\n",
    "        db = client[db_name]\n",
    "        print(\"MongoDB version:\", client.server_info()[\"version\"])\n",
    "        logger.info(f\"Connected to MongoDB database: {db_name}\")\n",
    "        return db\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to connect to MongoDB: {e}\")\n",
    "        raise\n",
    "\n",
    "db = create_mongodb_connection(mongodb_uri, 'stock_analytics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bff3ec8",
   "metadata": {},
   "source": [
    "### MongoDB Selection Rationale\n",
    "\n",
    "**Why MongoDB was chosen over other NoSQL options:**\n",
    "\n",
    "\n",
    "* **Document-oriented structure**\n",
    "\n",
    "  * MongoDB’s JSON-like document model naturally accommodates the semi-structured `stocktweet.csv` data (e.g., tweet text, ticker, and timestamp).\n",
    "  * No need for complex schema definitions or rigid table structures.\n",
    "\n",
    "* **Ease of use and developer-friendliness**\n",
    "  * It is easy to get started with MongoDB. The installation process is easy as well as connecting to the database\n",
    "  * MongoDB’s query language (MQL) is intuitive and similar to JSON syntax, which makes it easy to write.\n",
    "  * Rich developer tools such as MongoDB Compass which helps me visualize the database. E.g when I have created a collection, inserted a data, and so on.\n",
    "  * Extensive support for multiple programming languages (e.g., Python, Java, Scala).\n",
    "\n",
    "* **Seamless integration with Apache Spark**\n",
    "\n",
    "  * The `mongo-spark-connector` enables direct read/write access between MongoDB and Spark DataFrames.\n",
    "  * Simplifies data ingestion, distributed transformation, and analytics within the Spark environment.\n",
    "\n",
    "* **Efficient for read-heavy analytical workloads**\n",
    "\n",
    "  * MongoDB supports secondary indexing and text search—ideal for querying tweets by ticker symbol or date.\n",
    "  * Aggregation pipelines allow for efficient summarization and filtering of large datasets.\n",
    "\n",
    "* **Cloud accessibility and scalability**\n",
    "\n",
    "  * MongoDB Atlas provides a fully managed cloud database service that allows me to connect securely from anywhere.\n",
    "  * I can easily deploy and scale MongoDB clusters in the cloud and integrate them with my local Spark environment.\n",
    "  * Atlas ensures high availability, backup, and monitoring, which is ideal for handling large-scale, real-time tweet and stock data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009c1e26",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#2c3e54;color:#ecf0f1;border-radius: 8px; padding:15px\">Create Spark Session</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a539daeb-e1f7-4a8d-91fb-baf1b342f382",
   "metadata": {},
   "source": [
    "**Note:** The original MongoDB URI had the format `<mongodb+srv://<user>:<password>@cluster.mongodb.net/?...>`, which lacks a target database. Since Spark requires a database to be explicitly defined in the connection URI, the string was programmatically updated to include the `/stock_analytics` path before the query string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac55e975-9d04-4be3-ae84-a21d95d7cccc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:39:34.456786Z",
     "iopub.status.busy": "2025-07-14T18:39:34.455994Z",
     "iopub.status.idle": "2025-07-14T18:39:34.462218Z",
     "shell.execute_reply": "2025-07-14T18:39:34.461016Z",
     "shell.execute_reply.started": "2025-07-14T18:39:34.456756Z"
    }
   },
   "outputs": [],
   "source": [
    "# Insert 'stock_analytics' before the query string\n",
    "spark_mongodb_uri = mongodb_uri.replace(\".net/\", \".net/stock_analytics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cee31d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:39:34.463984Z",
     "iopub.status.busy": "2025-07-14T18:39:34.463703Z",
     "iopub.status.idle": "2025-07-14T18:39:34.497587Z",
     "shell.execute_reply": "2025-07-14T18:39:34.496036Z",
     "shell.execute_reply.started": "2025-07-14T18:39:34.463958Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_spark_session(app_name=\"StockAnalytics\"):\n",
    "    \"\"\"\n",
    "    Create Spark session with MongoDB connector\n",
    "    \n",
    "    Args:\n",
    "        app_name (str): Spark application name\n",
    "    \n",
    "    Returns:\n",
    "        pyspark.sql.SparkSession: Spark session instance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(app_name) \\\n",
    "            .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "            .config(\"spark.mongodb.input.uri\", spark_mongodb_uri) \\\n",
    "            .config(\"spark.mongodb.output.uri\", spark_mongodb_uri) \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        logger.info(f\"Spark session created: {app_name}\")\n",
    "        return spark\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create Spark session: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b4ed8b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:39:34.501490Z",
     "iopub.status.busy": "2025-07-14T18:39:34.501136Z",
     "iopub.status.idle": "2025-07-14T18:39:40.620207Z",
     "shell.execute_reply": "2025-07-14T18:39:40.618984Z",
     "shell.execute_reply.started": "2025-07-14T18:39:34.501463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.11/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e3ef89fe-374c-4de1-a492-75380e04545c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 317ms :: artifacts dl 18ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e3ef89fe-374c-4de1-a492-75380e04545c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/14ms)\n",
      "25/07/14 18:39:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2025-07-14 18:39:40,613 — INFO Spark session created: StockAnalytics\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "\n",
    "# Suppress Warnings in PySpark\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdde128-0041-45c1-84c5-c56e83dc3805",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T15:13:18.749667Z",
     "iopub.status.busy": "2025-07-14T15:13:18.749227Z",
     "iopub.status.idle": "2025-07-14T15:13:18.759738Z",
     "shell.execute_reply": "2025-07-14T15:13:18.758192Z",
     "shell.execute_reply.started": "2025-07-14T15:13:18.749636Z"
    }
   },
   "source": [
    "### Spark Selection Rationale\n",
    "\n",
    "**Why Spark instead of Hadoop MapReduce:**\n",
    "\n",
    "* Spark performs in-memory processing, making it much faster than Hadoop MapReduce—especially for repeated operations like filtering, joining, and aggregation.\n",
    "* It provides simpler, higher-level APIs (e.g., DataFrames and SQL) that are easier to use and more readable than the verbose MapReduce code.\n",
    "* Spark includes powerful built-in libraries for SQL, machine learning, and streaming, which are not natively available in MapReduce.\n",
    "\n",
    "**Why PySpark:**\n",
    "\n",
    "* PySpark allows me to use Python—making it easier to integrate with pandas, matplotlib, and other familiar libraries.\n",
    "* Its syntax is concise and readable, improving development speed and code clarity.\n",
    "* Strong community support and documentation simplify implementation and troubleshooting.\n",
    "\n",
    "**MongoDB Integration:**\n",
    "\n",
    "* The Spark session is configured with the MongoDB connector, allowing direct read/write access between Spark and MongoDB.\n",
    "* This setup creates a seamless pipeline from storage to distributed processing without intermediate conversions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf30a13-9272-4ae3-af14-81c1d2a03e11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T16:10:13.749924Z",
     "iopub.status.busy": "2025-07-14T16:10:13.749542Z",
     "iopub.status.idle": "2025-07-14T16:10:13.758266Z",
     "shell.execute_reply": "2025-07-14T16:10:13.756232Z",
     "shell.execute_reply.started": "2025-07-14T16:10:13.749894Z"
    }
   },
   "source": [
    "<h2 style=\"background-color:#2c3e54;color:#ecf0f1;border-radius: 8px; padding:15px\">Store Source Datasets into NoSQL Database using Spark</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c0b7252-24ec-404a-be65-a2ad91cae1a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:39:40.621844Z",
     "iopub.status.busy": "2025-07-14T18:39:40.621487Z",
     "iopub.status.idle": "2025-07-14T18:39:40.658480Z",
     "shell.execute_reply": "2025-07-14T18:39:40.657350Z",
     "shell.execute_reply.started": "2025-07-14T18:39:40.621802Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear existing data\n",
    "for collection_name in db.list_collection_names():\n",
    "    db[collection_name].drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33035061-ce6d-4f44-8f1c-13fd3a959e88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:39:40.659835Z",
     "iopub.status.busy": "2025-07-14T18:39:40.659561Z",
     "iopub.status.idle": "2025-07-14T18:40:33.269559Z",
     "shell.execute_reply": "2025-07-14T18:40:33.268133Z",
     "shell.execute_reply.started": "2025-07-14T18:39:40.659809Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 18:40:33,264 — INFO Loaded stock tweets into MongoDB using Spark     \n"
     ]
    }
   ],
   "source": [
    "def store_stock_tweets_to_mongodb_spark(spark, csv_path):\n",
    "    \"\"\"\n",
    "    Load stock tweet data from CSV to MongoDB using Spark\n",
    "    \n",
    "    Args:\n",
    "        spark (SparkSession): Spark session instance\n",
    "        csv_path (str): Path to stocktweet.csv file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.read.option(\"header\", True).csv(csv_path)\n",
    "        df.write.format(\"mongo\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"collection\", \"stock_tweets\") \\\n",
    "            .save()\n",
    "        \n",
    "        logger.info(f\"Loaded stock tweets into MongoDB using Spark\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load stock tweets via Spark: {e}\")\n",
    "        raise\n",
    "\n",
    "store_stock_tweets_to_mongodb_spark(spark, STOCKTWEET_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "150ad1c6-9cf3-4af1-8f5d-0dc8de8f1b2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:40:33.271535Z",
     "iopub.status.busy": "2025-07-14T18:40:33.271154Z",
     "iopub.status.idle": "2025-07-14T18:40:46.370503Z",
     "shell.execute_reply": "2025-07-14T18:40:46.369376Z",
     "shell.execute_reply.started": "2025-07-14T18:40:33.271506Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 18:40:46,366 — INFO Loaded stock prices into MongoDB using Spark     \n"
     ]
    }
   ],
   "source": [
    "def store_stock_prices_to_mongodb_spark(spark, stockprice_folder):\n",
    "    \"\"\"\n",
    "    Load stock price data from multiple CSVs to MongoDB using Spark\n",
    "    \n",
    "    Args:\n",
    "        spark (SparkSession): Spark session instance\n",
    "        stockprice_folder (str): Path to folder containing stock price CSV files\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load all CSVs from the folder\n",
    "        df = spark.read.option(\"header\", True).csv(f\"{stockprice_folder}/*.csv\")\n",
    "        df.write.format(\"mongo\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"collection\", \"stock_prices\") \\\n",
    "            .save()\n",
    "\n",
    "        logger.info(f\"Loaded stock prices into MongoDB using Spark\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load stock prices via Spark: {e}\")\n",
    "        raise\n",
    "\n",
    "store_stock_prices_to_mongodb_spark(spark, STOCKPRICE_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da33948c-a62c-4310-b555-bbd1a2355b12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T15:58:52.140546Z",
     "iopub.status.busy": "2025-07-14T15:58:52.140199Z",
     "iopub.status.idle": "2025-07-14T15:59:48.904390Z",
     "shell.execute_reply": "2025-07-14T15:59:48.903237Z",
     "shell.execute_reply.started": "2025-07-14T15:58:52.140513Z"
    }
   },
   "source": [
    "### Rationale for Using Spark to Populate MongoDB\n",
    "\n",
    "The source datasets (`stocktweet.csv` and stock price files) were loaded using **PySpark**, and written directly into a MongoDB NoSQL database using the **MongoDB Spark Connector**. This approach leverages Spark’s distributed capabilities to handle large data volumes efficiently and meets the requirement to populate the NoSQL database using a big data processing tool. Each dataset was written to a separate collection (`stock_tweets`, `stock_prices`) for streamlined querying and integration with further Spark-based analytics."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7150469,
     "sourceId": 11417157,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
