{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "923bcb38",
   "metadata": {},
   "source": [
    "<h1 align=\"center\" style=\"background-color:#2c3e54;color:#ecf0f1;border-radius: 8px; padding:15px\">MSc in Data Analytics: Big Data Storage and Processing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d805034",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "    - [Assessment Overview](#Assessment-Overview)\n",
    "    - [Project Summary](#Project-Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e0502b",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#2c3e54;color:#ecf0f1;border-radius: 8px; padding:15px\">Introduction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb5eed",
   "metadata": {},
   "source": [
    "### **Assessment Overview**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef855670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db0a1465",
   "metadata": {},
   "source": [
    "### Project Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184e2659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bce8a253",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#2c3e54;color:#ecf0f1;border-radius: 8px; padding:15px\">Install and Import Required Libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "302fe9f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:39:32.666380Z",
     "iopub.status.busy": "2025-07-14T18:39:32.665981Z",
     "iopub.status.idle": "2025-07-14T18:39:32.672666Z",
     "shell.execute_reply": "2025-07-14T18:39:32.671542Z",
     "shell.execute_reply.started": "2025-07-14T18:39:32.666350Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -q pyspark pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "709169ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:39:32.707810Z",
     "iopub.status.busy": "2025-07-14T18:39:32.707395Z",
     "iopub.status.idle": "2025-07-14T18:39:33.477834Z",
     "shell.execute_reply": "2025-07-14T18:39:33.476330Z",
     "shell.execute_reply.started": "2025-07-14T18:39:32.707774Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b8b63c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:39:33.481458Z",
     "iopub.status.busy": "2025-07-14T18:39:33.480131Z",
     "iopub.status.idle": "2025-07-14T18:39:33.487510Z",
     "shell.execute_reply": "2025-07-14T18:39:33.486279Z",
     "shell.execute_reply.started": "2025-07-14T18:39:33.481407Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s — %(levelname)s %(message)s', force=True)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Disable warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1b1730",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#2c3e54;color:#ecf0f1;border-radius: 8px; padding:15px\">Define Data Paths</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "264f9050",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:39:33.489524Z",
     "iopub.status.busy": "2025-07-14T18:39:33.489031Z",
     "iopub.status.idle": "2025-07-14T18:39:33.511870Z",
     "shell.execute_reply": "2025-07-14T18:39:33.510572Z",
     "shell.execute_reply.started": "2025-07-14T18:39:33.489471Z"
    }
   },
   "outputs": [],
   "source": [
    "STOCKPRICE_FOLDER = \"/kaggle/input/stock-tweet-and-price/stock-tweet-and-price/stockprice\"\n",
    "STOCKTWEET_CSV = \"/kaggle/input/stock-tweet-and-price/stock-tweet-and-price/stocktweet/stocktweet.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c158e95c",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#2c3e54;color:#ecf0f1;border-radius: 8px; padding:15px\">Set MongoDB Connection</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8393259",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:39:33.514598Z",
     "iopub.status.busy": "2025-07-14T18:39:33.514017Z",
     "iopub.status.idle": "2025-07-14T18:39:33.710304Z",
     "shell.execute_reply": "2025-07-14T18:39:33.708598Z",
     "shell.execute_reply.started": "2025-07-14T18:39:33.514560Z"
    }
   },
   "outputs": [],
   "source": [
    "user_secrets = UserSecretsClient()\n",
    "mongodb_uri = user_secrets.get_secret(\"mongodb-atlas-uri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e8afdbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:39:33.712587Z",
     "iopub.status.busy": "2025-07-14T18:39:33.712223Z",
     "iopub.status.idle": "2025-07-14T18:39:34.454531Z",
     "shell.execute_reply": "2025-07-14T18:39:34.453590Z",
     "shell.execute_reply.started": "2025-07-14T18:39:33.712558Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 18:39:34,449 — INFO Connected to MongoDB database: stock_analytics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB version: 8.0.11\n"
     ]
    }
   ],
   "source": [
    "def create_mongodb_connection(uri, db_name='stock_analytics'):\n",
    "    \"\"\"\n",
    "    Connect to MongoDB and return database instance\n",
    "    \n",
    "    Args:\n",
    "        uri (str): Database URI\n",
    "        db_name (str): Database name\n",
    "    \n",
    "    Returns:\n",
    "        pymongo.database.Database: MongoDB database instance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = MongoClient(uri)\n",
    "        db = client[db_name]\n",
    "        print(\"MongoDB version:\", client.server_info()[\"version\"])\n",
    "        logger.info(f\"Connected to MongoDB database: {db_name}\")\n",
    "        return db\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to connect to MongoDB: {e}\")\n",
    "        raise\n",
    "\n",
    "db = create_mongodb_connection(mongodb_uri, 'stock_analytics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bff3ec8",
   "metadata": {},
   "source": [
    "### MongoDB Selection Rationale\n",
    "\n",
    "**Why MongoDB was chosen over other NoSQL options:**\n",
    "\n",
    "\n",
    "* **Document-oriented structure**\n",
    "\n",
    "  * MongoDB’s JSON-like document model naturally accommodates the semi-structured `stocktweet.csv` data (e.g., tweet text, ticker, and timestamp).\n",
    "  * No need for complex schema definitions or rigid table structures.\n",
    "\n",
    "* **Ease of use and developer-friendliness**\n",
    "  * It is easy to get started with MongoDB. The installation process is easy as well as connecting to the database\n",
    "  * MongoDB’s query language (MQL) is intuitive and similar to JSON syntax, which makes it easy to write.\n",
    "  * Rich developer tools such as MongoDB Compass which helps me visualize the database. E.g when I have created a collection, inserted a data, and so on.\n",
    "  * Extensive support for multiple programming languages (e.g., Python, Java, Scala).\n",
    "\n",
    "* **Seamless integration with Apache Spark**\n",
    "\n",
    "  * The `mongo-spark-connector` enables direct read/write access between MongoDB and Spark DataFrames.\n",
    "  * Simplifies data ingestion, distributed transformation, and analytics within the Spark environment.\n",
    "\n",
    "* **Efficient for read-heavy analytical workloads**\n",
    "\n",
    "  * MongoDB supports secondary indexing and text search—ideal for querying tweets by ticker symbol or date.\n",
    "  * Aggregation pipelines allow for efficient summarization and filtering of large datasets.\n",
    "\n",
    "* **Cloud accessibility and scalability**\n",
    "\n",
    "  * MongoDB Atlas provides a fully managed cloud database service that allows me to connect securely from anywhere.\n",
    "  * I can easily deploy and scale MongoDB clusters in the cloud and integrate them with my local Spark environment.\n",
    "  * Atlas ensures high availability, backup, and monitoring, which is ideal for handling large-scale, real-time tweet and stock data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009c1e26",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#2c3e54;color:#ecf0f1;border-radius: 8px; padding:15px\">Create Spark Session</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a539daeb-e1f7-4a8d-91fb-baf1b342f382",
   "metadata": {},
   "source": [
    "**Note:** The original MongoDB URI had the format `<mongodb+srv://<user>:<password>@cluster.mongodb.net/?...>`, which lacks a target database. Since Spark requires a database to be explicitly defined in the connection URI, the string was programmatically updated to include the `/stock_analytics` path before the query string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac55e975-9d04-4be3-ae84-a21d95d7cccc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:39:34.456786Z",
     "iopub.status.busy": "2025-07-14T18:39:34.455994Z",
     "iopub.status.idle": "2025-07-14T18:39:34.462218Z",
     "shell.execute_reply": "2025-07-14T18:39:34.461016Z",
     "shell.execute_reply.started": "2025-07-14T18:39:34.456756Z"
    }
   },
   "outputs": [],
   "source": [
    "# Insert 'stock_analytics' before the query string\n",
    "spark_mongodb_uri = mongodb_uri.replace(\".net/\", \".net/stock_analytics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cee31d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:39:34.463984Z",
     "iopub.status.busy": "2025-07-14T18:39:34.463703Z",
     "iopub.status.idle": "2025-07-14T18:39:34.497587Z",
     "shell.execute_reply": "2025-07-14T18:39:34.496036Z",
     "shell.execute_reply.started": "2025-07-14T18:39:34.463958Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_spark_session(app_name=\"StockAnalytics\"):\n",
    "    \"\"\"\n",
    "    Create Spark session with MongoDB connector\n",
    "    \n",
    "    Args:\n",
    "        app_name (str): Spark application name\n",
    "    \n",
    "    Returns:\n",
    "        pyspark.sql.SparkSession: Spark session instance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(app_name) \\\n",
    "            .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "            .config(\"spark.mongodb.input.uri\", spark_mongodb_uri) \\\n",
    "            .config(\"spark.mongodb.output.uri\", spark_mongodb_uri) \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        logger.info(f\"Spark session created: {app_name}\")\n",
    "        return spark\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create Spark session: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b4ed8b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:39:34.501490Z",
     "iopub.status.busy": "2025-07-14T18:39:34.501136Z",
     "iopub.status.idle": "2025-07-14T18:39:40.620207Z",
     "shell.execute_reply": "2025-07-14T18:39:40.618984Z",
     "shell.execute_reply.started": "2025-07-14T18:39:34.501463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.11/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e3ef89fe-374c-4de1-a492-75380e04545c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 317ms :: artifacts dl 18ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e3ef89fe-374c-4de1-a492-75380e04545c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/14ms)\n",
      "25/07/14 18:39:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2025-07-14 18:39:40,613 — INFO Spark session created: StockAnalytics\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "\n",
    "# Suppress Warnings in PySpark\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdde128-0041-45c1-84c5-c56e83dc3805",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T15:13:18.749667Z",
     "iopub.status.busy": "2025-07-14T15:13:18.749227Z",
     "iopub.status.idle": "2025-07-14T15:13:18.759738Z",
     "shell.execute_reply": "2025-07-14T15:13:18.758192Z",
     "shell.execute_reply.started": "2025-07-14T15:13:18.749636Z"
    }
   },
   "source": [
    "### Spark Selection Rationale\n",
    "\n",
    "**Why Spark instead of Hadoop MapReduce:**\n",
    "\n",
    "* Spark performs in-memory processing, making it much faster than Hadoop MapReduce—especially for repeated operations like filtering, joining, and aggregation.\n",
    "* It provides simpler, higher-level APIs (e.g., DataFrames and SQL) that are easier to use and more readable than the verbose MapReduce code.\n",
    "* Spark includes powerful built-in libraries for SQL, machine learning, and streaming, which are not natively available in MapReduce.\n",
    "\n",
    "**Why PySpark:**\n",
    "\n",
    "* PySpark allows me to use Python—making it easier to integrate with pandas, matplotlib, and other familiar libraries.\n",
    "* Its syntax is concise and readable, improving development speed and code clarity.\n",
    "* Strong community support and documentation simplify implementation and troubleshooting.\n",
    "\n",
    "**MongoDB Integration:**\n",
    "\n",
    "* The Spark session is configured with the MongoDB connector, allowing direct read/write access between Spark and MongoDB.\n",
    "* This setup creates a seamless pipeline from storage to distributed processing without intermediate conversions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf30a13-9272-4ae3-af14-81c1d2a03e11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T16:10:13.749924Z",
     "iopub.status.busy": "2025-07-14T16:10:13.749542Z",
     "iopub.status.idle": "2025-07-14T16:10:13.758266Z",
     "shell.execute_reply": "2025-07-14T16:10:13.756232Z",
     "shell.execute_reply.started": "2025-07-14T16:10:13.749894Z"
    }
   },
   "source": [
    "<h2 style=\"background-color:#2c3e54;color:#ecf0f1;border-radius: 8px; padding:15px\">Store Source Datasets into NoSQL Database using Spark</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c0b7252-24ec-404a-be65-a2ad91cae1a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:39:40.621844Z",
     "iopub.status.busy": "2025-07-14T18:39:40.621487Z",
     "iopub.status.idle": "2025-07-14T18:39:40.658480Z",
     "shell.execute_reply": "2025-07-14T18:39:40.657350Z",
     "shell.execute_reply.started": "2025-07-14T18:39:40.621802Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear existing data\n",
    "for collection_name in db.list_collection_names():\n",
    "    db[collection_name].drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33035061-ce6d-4f44-8f1c-13fd3a959e88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:39:40.659835Z",
     "iopub.status.busy": "2025-07-14T18:39:40.659561Z",
     "iopub.status.idle": "2025-07-14T18:40:33.269559Z",
     "shell.execute_reply": "2025-07-14T18:40:33.268133Z",
     "shell.execute_reply.started": "2025-07-14T18:39:40.659809Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 18:40:33,264 — INFO Loaded stock tweets into MongoDB using Spark     \n"
     ]
    }
   ],
   "source": [
    "def store_stock_tweets_to_mongodb_spark(spark, csv_path):\n",
    "    \"\"\"\n",
    "    Load stock tweet data from CSV to MongoDB using Spark\n",
    "    \n",
    "    Args:\n",
    "        spark (SparkSession): Spark session instance\n",
    "        csv_path (str): Path to stocktweet.csv file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.read.option(\"header\", True).csv(csv_path)\n",
    "        df.write.format(\"mongo\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"collection\", \"stock_tweets\") \\\n",
    "            .save()\n",
    "        \n",
    "        logger.info(f\"Loaded stock tweets into MongoDB using Spark\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load stock tweets via Spark: {e}\")\n",
    "        raise\n",
    "\n",
    "store_stock_tweets_to_mongodb_spark(spark, STOCKTWEET_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "150ad1c6-9cf3-4af1-8f5d-0dc8de8f1b2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:40:33.271535Z",
     "iopub.status.busy": "2025-07-14T18:40:33.271154Z",
     "iopub.status.idle": "2025-07-14T18:40:46.370503Z",
     "shell.execute_reply": "2025-07-14T18:40:46.369376Z",
     "shell.execute_reply.started": "2025-07-14T18:40:33.271506Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 18:40:46,366 — INFO Loaded stock prices into MongoDB using Spark     \n"
     ]
    }
   ],
   "source": [
    "def store_stock_prices_to_mongodb_spark(spark, stockprice_folder):\n",
    "    \"\"\"\n",
    "    Load stock price data from multiple CSVs to MongoDB using Spark\n",
    "    \n",
    "    Args:\n",
    "        spark (SparkSession): Spark session instance\n",
    "        stockprice_folder (str): Path to folder containing stock price CSV files\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load all CSVs from the folder\n",
    "        df = spark.read.option(\"header\", True).csv(f\"{stockprice_folder}/*.csv\")\n",
    "        df.write.format(\"mongo\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"collection\", \"stock_prices\") \\\n",
    "            .save()\n",
    "\n",
    "        logger.info(f\"Loaded stock prices into MongoDB using Spark\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load stock prices via Spark: {e}\")\n",
    "        raise\n",
    "\n",
    "store_stock_prices_to_mongodb_spark(spark, STOCKPRICE_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da33948c-a62c-4310-b555-bbd1a2355b12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T15:58:52.140546Z",
     "iopub.status.busy": "2025-07-14T15:58:52.140199Z",
     "iopub.status.idle": "2025-07-14T15:59:48.904390Z",
     "shell.execute_reply": "2025-07-14T15:59:48.903237Z",
     "shell.execute_reply.started": "2025-07-14T15:58:52.140513Z"
    }
   },
   "source": [
    "### Rationale for Using Spark to Populate MongoDB\n",
    "\n",
    "The source datasets (`stocktweet.csv` and stock price files) were loaded using **PySpark**, and written directly into a MongoDB NoSQL database using the **MongoDB Spark Connector**. This approach leverages Spark’s distributed capabilities to handle large data volumes efficiently and meets the requirement to populate the NoSQL database using a big data processing tool. Each dataset was written to a separate collection (`stock_tweets`, `stock_prices`) for streamlined querying and integration with further Spark-based analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac6aab3-f7e6-4e4b-a406-fead8b77c847",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#2c3e54;color:#ecf0f1;border-radius: 8px; padding:15px\">Read Source Datasets into NoSQL Database using Spark</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ce04a73-5002-475d-90c3-8a41e2cce9a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:40:46.372168Z",
     "iopub.status.busy": "2025-07-14T18:40:46.371785Z",
     "iopub.status.idle": "2025-07-14T18:40:49.845820Z",
     "shell.execute_reply": "2025-07-14T18:40:49.837887Z",
     "shell.execute_reply.started": "2025-07-14T18:40:46.372130Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 18:40:49,819 — INFO Read 11149 tweets from MongoDB                   \n"
     ]
    }
   ],
   "source": [
    "def read_tweets_from_mongodb(spark):\n",
    "    \"\"\"\n",
    "    Read tweet data from MongoDB using Spark\n",
    "    \n",
    "    Args:\n",
    "        spark: Spark session instance\n",
    "    \n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: Spark DataFrame containing tweet data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.read \\\n",
    "            .format(\"mongo\") \\\n",
    "            .option(\"collection\", \"stock_tweets\") \\\n",
    "            .load()\n",
    "        \n",
    "        logger.info(f\"Read {df.count()} tweets from MongoDB\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read tweets from MongoDB: {e}\")\n",
    "        raise\n",
    "\n",
    "tweets_df = read_tweets_from_mongodb(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c2381b8-21b9-47dc-b5e3-21ed78f2db05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:40:49.847180Z",
     "iopub.status.busy": "2025-07-14T18:40:49.846804Z",
     "iopub.status.idle": "2025-07-14T18:40:51.816237Z",
     "shell.execute_reply": "2025-07-14T18:40:51.815160Z",
     "shell.execute_reply.started": "2025-07-14T18:40:49.847150Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 18:40:51,809 — INFO Read 10175 price records from MongoDB\n"
     ]
    }
   ],
   "source": [
    "def read_prices_from_mongodb(spark):\n",
    "    \"\"\"\n",
    "    Read stock price data from MongoDB using Spark\n",
    "    \n",
    "    Args:\n",
    "        spark: Spark session instance\n",
    "    \n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: Spark DataFrame containing price data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.read \\\n",
    "            .format(\"mongo\") \\\n",
    "            .option(\"collection\", \"stock_prices\") \\\n",
    "            .load()\n",
    "        \n",
    "        logger.info(f\"Read {df.count()} price records from MongoDB\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read prices from MongoDB: {e}\")\n",
    "        raise\n",
    "\n",
    "prices_df = read_prices_from_mongodb(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a121ad-3cb1-44ec-b476-e27891799970",
   "metadata": {},
   "source": [
    "### Rationale for Using Spark to Read from MongoDB\n",
    "\n",
    "**Data Retrieval (Read):**\n",
    "The `read_tweets_from_mongodb()` and `read_prices_from_mongodb()` functions use Spark’s MongoDB connector to directly query collections into Spark DataFrames. This enables seamless integration for downstream analytics, leveraging Spark’s speed and MongoDB’s flexible document storage.\n",
    "\n",
    "**Why this matters:**\n",
    "Using Spark for both reading and writing ensures a **unified, high-performance pipeline** between storage (MongoDB) and processing (Spark), ideal for handling large-scale tweet and stock data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857d0d68-79bd-4c44-8291-48372b2eba26",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#2c3e54;color:#ecf0f1;border-radius: 8px; padding:15px\">Process tweet sentiment</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9918ac81-569d-496f-8fb2-436165f1f16f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:40:51.817473Z",
     "iopub.status.busy": "2025-07-14T18:40:51.817114Z",
     "iopub.status.idle": "2025-07-14T18:40:51.827857Z",
     "shell.execute_reply": "2025-07-14T18:40:51.825930Z",
     "shell.execute_reply.started": "2025-07-14T18:40:51.817438Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_keywords = [\n",
    "    \"up\", \"bull\", \"bullish\", \"gain\", \"gains\", \"rise\", \"rising\", \"high\", \"higher\", \"buy\", \"strong\", \n",
    "    \"positive\", \"green\", \"profit\", \"profits\", \"surge\", \"surging\", \"beat\", \"beats\", \"boom\", \"breakout\",\n",
    "    \"support\", \"soar\", \"soaring\", \"rocket\", \"growth\", \"rebound\", \"optimistic\", \"win\", \"wins\", \"recovery\"\n",
    "]\n",
    "\n",
    "negative_keywords = [\n",
    "    \"down\", \"bear\", \"bearish\", \"loss\", \"losses\", \"fall\", \"falling\", \"low\", \"lower\", \"sell\", \"selling\", \n",
    "    \"weak\", \"negative\", \"red\", \"drop\", \"dropped\", \"dip\", \"dipping\", \"miss\", \"missed\", \"crash\", \"plunge\",\n",
    "    \"resistance\", \"collapse\", \"tank\", \"tanking\", \"uncertain\", \"volatile\", \"recession\", \"fear\", \"panic\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51b11a66-ffe6-44ec-9416-46c163a73065",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:40:51.829353Z",
     "iopub.status.busy": "2025-07-14T18:40:51.828813Z",
     "iopub.status.idle": "2025-07-14T18:40:51.859862Z",
     "shell.execute_reply": "2025-07-14T18:40:51.858533Z",
     "shell.execute_reply.started": "2025-07-14T18:40:51.829301Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_tweet_sentiment_analysis(tweets_df, positive_keywords, negative_keywords):\n",
    "    \"\"\"\n",
    "    Process tweets for basic sentiment analysis and aggregation\n",
    "    \n",
    "    Args:\n",
    "        tweets_df (pyspark.sql.DataFrame): Spark DataFrame containing tweet data\n",
    "    \n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: Processed tweet data with sentiment indicators\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure date column is in proper format\n",
    "        tweets_df = tweets_df.withColumn(\"date\", to_date(col(\"date\"), \"MM/dd/yyyy\"))\n",
    "    \n",
    "        # Normalize tweet text\n",
    "        tweets_df = tweets_df.withColumn(\"tweet_clean\", lower(regexp_replace(col(\"tweet\"), r\"[^a-zA-Z\\s]\", \"\")))\n",
    "    \n",
    "        # Use regex for case-insensitive keyword matching\n",
    "        pos_regex = r\"\\b(\" + \"|\".join(positive_keywords) + r\")\\b\"\n",
    "        neg_regex = r\"\\b(\" + \"|\".join(negative_keywords) + r\")\\b\"\n",
    "    \n",
    "        # Compute sentiment scores\n",
    "        tweets_df = tweets_df.withColumn(\"positive_sentiment\", when(col(\"tweet_clean\").rlike(pos_regex), 1).otherwise(0))\n",
    "        tweets_df = tweets_df.withColumn(\"negative_sentiment\", when(col(\"tweet_clean\").rlike(neg_regex), 1).otherwise(0))\n",
    "        tweets_df = tweets_df.withColumn(\"net_sentiment\", col(\"positive_sentiment\") - col(\"negative_sentiment\"))\n",
    "    \n",
    "        # Sentiment label for readability\n",
    "        tweets_df = tweets_df.withColumn(\n",
    "            \"sentiment_label\",\n",
    "            when(col(\"net_sentiment\") > 0, \"positive\")\n",
    "            .when(col(\"net_sentiment\") < 0, \"negative\")\n",
    "            .otherwise(\"neutral\")\n",
    "        )\n",
    "    \n",
    "        # Drop temporary columns\n",
    "        tweets_df = tweets_df.drop(\"tweet_clean\")\n",
    "    \n",
    "        logger.info(\"Enhanced tweet sentiment analysis completed\")\n",
    "        return tweets_df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to process tweet sentiment: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69aff71e-aeb1-4de0-92df-5340954cde31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:40:51.861888Z",
     "iopub.status.busy": "2025-07-14T18:40:51.861350Z",
     "iopub.status.idle": "2025-07-14T18:40:52.133653Z",
     "shell.execute_reply": "2025-07-14T18:40:52.132383Z",
     "shell.execute_reply.started": "2025-07-14T18:40:51.861844Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 18:40:51,891 — INFO Processing tweet sentiment analysis...\n",
      "2025-07-14 18:40:52,128 — INFO Enhanced tweet sentiment analysis completed\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Processing tweet sentiment analysis...\")\n",
    "processed_tweets = process_tweet_sentiment_analysis(tweets_df, positive_keywords, negative_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8821035-cad3-48a2-b7d7-06d409cfaea2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:40:52.135536Z",
     "iopub.status.busy": "2025-07-14T18:40:52.135020Z",
     "iopub.status.idle": "2025-07-14T18:40:53.590358Z",
     "shell.execute_reply": "2025-07-14T18:40:53.589276Z",
     "shell.execute_reply.started": "2025-07-14T18:40:52.135495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------+------+--------------------+------------------+------------------+-------------+---------------+\n",
      "|                 _id|      date|    id|ticker|               tweet|positive_sentiment|negative_sentiment|net_sentiment|sentiment_label|\n",
      "+--------------------+----------+------+------+--------------------+------------------+------------------+-------------+---------------+\n",
      "|{68754ef506c5f159...|2020-01-01|100001|  AMZN|$AMZN Dow futures...|                 1|                 0|            1|       positive|\n",
      "|{68754ef506c5f159...|2020-01-01|100002|  TSLA|$TSLA Daddy's dri...|                 0|                 0|            0|        neutral|\n",
      "|{68754ef506c5f159...|2020-01-01|100003|  AAPL|$AAPL We’ll been ...|                 0|                 0|            0|        neutral|\n",
      "+--------------------+----------+------+------+--------------------+------------------+------------------+-------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_tweets.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cacf3bf-b590-4bd3-bfb8-262d3683686f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T16:47:14.021364Z",
     "iopub.status.busy": "2025-07-14T16:47:14.020775Z",
     "iopub.status.idle": "2025-07-14T16:47:14.040944Z",
     "shell.execute_reply": "2025-07-14T16:47:14.038393Z",
     "shell.execute_reply.started": "2025-07-14T16:47:14.021324Z"
    }
   },
   "source": [
    "### Spark-Based Tweet Sentiment Processing: Rationale and Justification\n",
    "\n",
    "To extract actionable insights from unstructured social media content, we implemented a **basic rule-based sentiment analysis pipeline** using **PySpark**, designed to classify tweets related to stock tickers as positive, negative, or neutral. This approach leverages the **distributed computing power of Apache Spark** to handle large-scale textual data efficiently and integrates seamlessly with MongoDB for scalable read-write operations.\n",
    "\n",
    "#### 1. **Why Spark for Sentiment Processing?**\n",
    "\n",
    "* **Scalability and Distributed Execution**: Given the volume of tweet data (>10,000 records), Spark's distributed in-memory processing is optimal for parallelizing transformations such as string cleaning, keyword matching, and column derivation without bottlenecks.\n",
    "* **SQL-like DataFrame API**: Spark DataFrames offer a high-level abstraction that allows expressive, readable, and efficient transformations, well-suited for text pre-processing and sentiment scoring logic.\n",
    "* **Seamless MongoDB Integration**: Since data was stored in MongoDB, Spark's native Mongo connector allows efficient data ingestion and storage without requiring intermediate formats.\n",
    "\n",
    "#### 2. **Justification for Rule-Based Sentiment Analysis**\n",
    "\n",
    "While more complex models like deep learning classifiers could be used, the project prioritizes simplicity and runtime efficiency within a Spark pipeline. A **dictionary-based sentiment approach** using predefined `positive_keywords` and `negative_keywords` allows for:\n",
    "\n",
    "* **Fast pattern matching** using Spark’s `rlike()` method and regular expressions.\n",
    "* **Lightweight processing** suited to the project’s scale and distributed context.\n",
    "* **Transparent results** where each tweet’s label (positive/negative/neutral) can be traced back to keyword presence.\n",
    "\n",
    "#### 3. **Key Design Choices**\n",
    "\n",
    "* **Text Normalization**: Special characters were removed and tweet content was lowercased using `regexp_replace()` and `lower()` to ensure consistent pattern matching.\n",
    "* **Regex-based Classification**: The method supports partial word matches and avoids false positives through word boundary anchors (`\\b`) in the regex pattern.\n",
    "* **Sentiment Scoring Logic**:\n",
    "\n",
    "  * `positive_sentiment` and `negative_sentiment` are binary indicators.\n",
    "  * `net_sentiment` provides a numeric sentiment score.\n",
    "  * `sentiment_label` provides a human-readable category for downstream use (e.g., grouping, visualization).\n",
    "* **Date Conversion**: The `date` column is parsed using `to_date()` to enable accurate time-series grouping or trend analysis.\n",
    "\n",
    "#### 4. **Alignment with Big Data Objectives**\n",
    "\n",
    "This implementation supports **real-time data processing scenarios**, as it can be adapted to Spark Streaming. It also supports **follow-up analytical steps** like joining with stock price data on the `date` and `ticker` fields for correlation analysis or event-driven stock movement prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab138d3-6830-4c09-bd33-d172344cdf16",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#2c3e54;color:#ecf0f1;border-radius: 8px; padding:15px\">Process stock price metrics</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f1dcd8e-fb90-47fe-8dbd-60784f02bf17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:40:53.591657Z",
     "iopub.status.busy": "2025-07-14T18:40:53.591363Z",
     "iopub.status.idle": "2025-07-14T18:40:53.602155Z",
     "shell.execute_reply": "2025-07-14T18:40:53.601125Z",
     "shell.execute_reply.started": "2025-07-14T18:40:53.591629Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_stock_price_metrics(prices_df):\n",
    "    \"\"\"\n",
    "    Process stock price data to calculate key metrics\n",
    "    \n",
    "    Args:\n",
    "        prices_df: Spark DataFrame containing price data\n",
    "    \n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: Processed price data with calculated metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert date string to proper date format\n",
    "        prices_df = prices_df.withColumn(\"date\", to_date(col(\"Date\"), \"yyyy-MM-dd\"))\n",
    "        \n",
    "        # Calculate daily price change and percentage change\n",
    "        prices_df = prices_df.withColumn(\"daily_change\", col(\"Close\") - col(\"Open\"))\n",
    "        prices_df = prices_df.withColumn(\"daily_change_pct\", \n",
    "                                       (col(\"daily_change\") / col(\"Open\")) * 100)\n",
    "        \n",
    "        # Calculate daily volatility (High - Low)\n",
    "        prices_df = prices_df.withColumn(\"daily_volatility\", col(\"High\") - col(\"Low\"))\n",
    "        prices_df = prices_df.withColumn(\"volatility_pct\", \n",
    "                                       (col(\"daily_volatility\") / col(\"Open\")) * 100)\n",
    "        \n",
    "        # Calculate volume-weighted average price (VWAP)\n",
    "        prices_df = prices_df.withColumn(\"vwap\", \n",
    "                                       (col(\"High\") + col(\"Low\") + col(\"Close\")) / 3)\n",
    "        \n",
    "        logger.info(\"Stock price metrics calculation completed\")\n",
    "        return prices_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to process stock price metrics: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1343355b-07bb-439b-a85c-0698e9baea69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:40:53.607540Z",
     "iopub.status.busy": "2025-07-14T18:40:53.606883Z",
     "iopub.status.idle": "2025-07-14T18:40:53.795939Z",
     "shell.execute_reply": "2025-07-14T18:40:53.790591Z",
     "shell.execute_reply.started": "2025-07-14T18:40:53.607503Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 18:40:53,621 — INFO Processing stock price metrics...\n",
      "2025-07-14 18:40:53,785 — INFO Stock price metrics calculation completed\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Processing stock price metrics...\")\n",
    "processed_prices = process_stock_price_metrics(prices_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62909050-59fc-4ff4-9926-b2cc8849fbd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:40:53.799673Z",
     "iopub.status.busy": "2025-07-14T18:40:53.797215Z",
     "iopub.status.idle": "2025-07-14T18:40:54.961873Z",
     "shell.execute_reply": "2025-07-14T18:40:54.960779Z",
     "shell.execute_reply.started": "2025-07-14T18:40:53.799615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+----------+-----------------+-----------------+----------------+--------+--------------------+-------------------+-------------------+------------------+------------------+-----------------+\n",
      "|        Adj Close|            Close|      date|             High|              Low|            Open|  Volume|                 _id|       daily_change|   daily_change_pct|  daily_volatility|    volatility_pct|             vwap|\n",
      "+-----------------+-----------------+----------+-----------------+-----------------+----------------+--------+--------------------+-------------------+-------------------+------------------+------------------+-----------------+\n",
      "|92.39199829101562|92.39199829101562|2019-12-31|92.66300201416016|91.61150360107422|92.0999984741211|50130000|{68754f2306c5f159...|0.29199981689453125|0.31704649482331904|1.0514984130859375|1.1416921069563264|   92.22216796875|\n",
      "|94.90049743652344|94.90049743652344|2020-01-02|94.90049743652344| 93.2074966430664|           93.75|80580000|{68754f2306c5f159...| 1.1504974365234375| 1.2271972656249999|1.6930007934570312|1.8058675130208333|94.33616383870442|\n",
      "|93.74849700927734|93.74849700927734|2020-01-03|94.30999755859375| 93.2249984741211|93.2249984741211|75288000|{68754f2306c5f159...|   0.52349853515625| 0.5615430879321186|1.0849990844726562| 1.163849935351673|93.76116434733073|\n",
      "+-----------------+-----------------+----------+-----------------+-----------------+----------------+--------+--------------------+-------------------+-------------------+------------------+------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_prices.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a013960-bb0a-4fa8-be50-e853a20d95da",
   "metadata": {},
   "source": [
    "### Stock Price Metrics Processing: Rationale and Justification\n",
    "\n",
    "To extract meaningful insights from historical stock data, this function computes key financial metrics using **PySpark**, enabling distributed and scalable processing of multiple CSV files across 38 tickers.\n",
    "\n",
    "#### Why PySpark?\n",
    "\n",
    "* **Distributed Efficiency**: PySpark efficiently handles large, multi-file datasets, enabling parallel computation of metrics like volatility and price movement without memory bottlenecks.\n",
    "* **SQL-like API**: PySpark’s DataFrame API simplifies column-wise operations and supports complex transformations in an expressive, readable manner.\n",
    "\n",
    "#### Calculated Metrics\n",
    "\n",
    "* **Daily Change** (`Close - Open`) and **% Change** provide insight into daily stock performance.\n",
    "* **Volatility** (`High - Low`) and **% Volatility** measure market fluctuations.\n",
    "* **VWAP** (Volume Weighted Average Price approximation using `(High + Low + Close)/3`) offers a benchmark for intraday price assessment.\n",
    "\n",
    "#### Practical Use\n",
    "\n",
    "These metrics can support further analysis, such as correlating price movement with tweet sentiment or identifying high-volatility stocks. Processing is aligned with big data principles by avoiding sequential or in-memory operations."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7150469,
     "sourceId": 11417157,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
